# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The dataset contains data about potential new clients for a bank. In trying to acquire new clients by the bank this data was recorded, but we seek to predict the success rate in their marketing tactic if the said client will accept conducting business with the bank.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

Both the automl and hyperdrive methods were employed, with the former slightly having a better accuracy of 0.918, with the best performing model being the VotingEnsemble, while the latter had a maximum of 0.908 in accuracy training with Logistic Regression.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The dataset is created by accessing and retrieving from the URL given and stored in a tabular dataset using the TabularDatasetFactory. It then goes through the data preprocessing, cleaning and transformation process to obtain a reliable dataset that helps achieve the best result in training the model. A dictionary was created and the values in months and weekdays were assigned to their corresponding numeric values. Also, the one hot Encoding was employed to preprocess the categorical features in the dataset. The resulting data is split into the test and training set.
A model is defined using the logistic regression algorithm integrated from Sci-kit learn to train the data by tuning the hyperparameters, "C" and "max_iter", which are the inverse of regularization strength and Maximum number of iterations to converge, respectively. 
For the hyperdrive to validly run the parameter sampler, policy, environment, script run config and hyperdrive config was specified. Accuracy is set as the primary metric, and the best run is displayed and stored as the 'sklearn_log_reg_train_P' model. 

**What are the benefits of the parameter sampler you chose?**
Random sampling was selected over the Grid sampling because it always chooses random values that help reduce the time needed for computation, unlike the latter that searches through the entire grid and was also chosen over the bayesian sampling because it supports an early stopping policy.  

**What are the benefits of the early stopping policy you chose?**
The early stopping policy chosen was the Bandit Policy. The method reduces the computation time needed to complete the hyperdrive run. It is booted when the primary metric is not within range of the slack factor and the best performing run for an interval I set to 4. 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
With over 20 models having above 90% accuracy, the best recorded was the VotingEnsemble with an accuracy of 0.918. An ensemble acting as a group had some of the algorithms 'LightGBM', XGBoostClasifier', 'SGD' and 'LogisticRegression', coupled with different data transformation methods with each ensemble having its weights and hyperparameters. After, an explanation was generated by automl and an observation was made that the most important features to the model prediction were 'duration', 'nr.employed', 'cons.conf.idx' and euribor3m' features with the duration feature scoring the highest with importance rating of 0.538. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
Both models performed similarly with the VotingEnsemble(automl) having an accuracy of 0.917 and the LogisticRegression(hyperdrive), having an accuracy of 0.908, but in terms of architecture, the automl proved simpler to code provided that it had an output of multiple models and slightly higher accuracy. It would take much more effort, time and resource trying to produce the same number of models using the hyperdrive configuration.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
During the run, I was alerted on Class Imbalanced Detection, which indicated that the run be cancelled, and the balancing problem should be fixed because the input data was biased towards one class, which could affect the accuracy of the model. Solving this problem will help reduce bias in the model, and in the future cannot generalize the dataset based on an unseen data point.
